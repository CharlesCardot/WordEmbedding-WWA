{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Women Writing Africa, Data Mining Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from importlib import reload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert PDF to txt document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = Path(\"Datasets\") / \"women writing africa eastern region.pdf\"\n",
    "out_txt_path = Path(\"Datasets\") / \"women writing africa eastern region.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needs to be run once\n",
    "reload(ut)\n",
    "ut.pdftotxt(pdf_path, out_txt_path, progress_updates=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse txt document\n",
    "\n",
    "Goals:\n",
    "- Identify content pattern, ex: (Author Name -> Title -> Country, Year, Language) precedes all content blocks\n",
    "- Extract individual documents, including forewards\n",
    "- Devise method for removing forwards\n",
    "- Create a document database of Metadata: Content\n",
    "\n",
    "Extraction pattern examples:\n",
    "\n",
    "```\n",
    "1.\n",
    "Siti binti Saad\n",
    "FOUR SONGS\n",
    "Tanzania 1920s Kiswahili\n",
    "\n",
    "2.\n",
    "Nellie Grant\n",
    "\n",
    "LETTERS FROM AFRICA TO A DAUGHTER\n",
    "IN ENGLAND\n",
    "\n",
    "Kenya 1939-1963 English\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index is 4535\n",
      "End Index is 23579\n"
     ]
    }
   ],
   "source": [
    "reload(ut)\n",
    "with open(out_txt_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "country_list = [\"Tanzania\", \"Kenya\", \"Malawi\", \"Zambia\", \"Uganda\", \"Swaziland\"]\n",
    "\n",
    "start_phrase = [\"Sultan Fatima binti Muhammad Mkubwa\\n\", \"PEACE AND SECURITY\\n\", \"Tanzania 1711 Kiswahili\\n\"]\n",
    "end_phrase = [\"CONTRIBUTORS\\n\", \"\\n\", \"EDITORS\\n\", \"\\n\"]\n",
    "\n",
    "for i in range(len(lines)-3):\n",
    "    if lines[i:i+3] == start_phrase:\n",
    "        print(\"Start Index is\", i)\n",
    "        start_index = i\n",
    "    if lines[i:i+4] == end_phrase:\n",
    "        print(\"End Index is\", i)\n",
    "        end_index = i\n",
    "lines = lines[start_index - 10:end_index-3]\n",
    "\n",
    "marker_string_indices = []\n",
    "for i in range(len(lines)-2):\n",
    "        \n",
    "    # Checks to see if string contains a year between 1700 and 2099\n",
    "    year_check = ut.contains_year(lines[i], year_min=\"1600\", year_max=\"2099\")\n",
    "    \n",
    "    if year_check:\n",
    "        \n",
    "        # Checks to see if string contains an approved country name\n",
    "        contains_country = []\n",
    "        for country in country_list:\n",
    "            if country in lines[i]:\n",
    "                contains_country.append(True)\n",
    "            else:\n",
    "                contains_country.append(False)\n",
    "\n",
    "        if any(contains_country):\n",
    "            \n",
    "            # Checks to make sure string has appropriate number of words\n",
    "            if 2 < len(lines[i].split()) < 6:\n",
    "                marker_string_indices.append(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse into Header, Chunk Foreward, and Chunk Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index is 4535\n",
      "End Index is 23579\n"
     ]
    }
   ],
   "source": [
    "reload(ut)\n",
    "with open(out_txt_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "country_list = [\"Tanzania\", \"Kenya\", \"Malawi\", \"Zambia\", \"Uganda\", \"Swaziland\"]\n",
    "\n",
    "start_phrase = [\"Sultan Fatima binti Muhammad Mkubwa\\n\", \"PEACE AND SECURITY\\n\", \"Tanzania 1711 Kiswahili\\n\"]\n",
    "end_phrase = [\"CONTRIBUTORS\\n\", \"\\n\", \"EDITORS\\n\", \"\\n\"]\n",
    "\n",
    "for i in range(len(lines)-3):\n",
    "    if lines[i:i+3] == start_phrase:\n",
    "        print(\"Start Index is\", i)\n",
    "        start_index = i\n",
    "    if lines[i:i+4] == end_phrase:\n",
    "        print(\"End Index is\", i)\n",
    "        end_index = i\n",
    "\n",
    "lines = lines[start_index - 5:end_index-3]\n",
    "for key, val in enumerate(lines):\n",
    "    if \"\\n\" in val and len(val) < 6:\n",
    "        lines[key] = \"\\n\"\n",
    "\n",
    "marker_string_indices = []\n",
    "for i in range(len(lines)-2):\n",
    "        \n",
    "    # Checks to see if string contains a year between 1700 and 2099\n",
    "    year_check = ut.contains_year(lines[i], year_min=\"1600\", year_max=\"2099\")\n",
    "    \n",
    "    if year_check:\n",
    "        \n",
    "        # Checks to see if string contains an approved country name\n",
    "        contains_country = []\n",
    "        for country in country_list:\n",
    "            if country in lines[i]:\n",
    "                contains_country.append(True)\n",
    "            else:\n",
    "                contains_country.append(False)\n",
    "\n",
    "        if any(contains_country):\n",
    "            \n",
    "            # Checks to make sure string has appropriate number of words\n",
    "            if 2 < len(lines[i].split()) < 6:\n",
    "                marker_string_indices.append(i)\n",
    "\n",
    "def header_parse(header_group):\n",
    "    arr = [0 if i == \"\\n\" else 1 for i in header_group]\n",
    "    simple_formats = [\n",
    "        [1,0,1,1,0,1], [1,0,1,0,1,1], [1,0,1,1,1,0], [1,0,1,0,1,0]\n",
    "    ]\n",
    "    # Most Common\n",
    "    if np.sum(arr[0:3]) == 3:\n",
    "        return header_group[0:3]\n",
    "    # Simple Formats\n",
    "    elif any(arr == i for i in simple_formats):\n",
    "        return [header_group[i] for i in range(len(arr)) if arr[i]==1]\n",
    "    # More Complex Formats\n",
    "    elif arr == [1,0,1,1,1,1]: \n",
    "        return [header_group[0], header_group[2], header_group[3]]\n",
    "    elif arr == [1,1,0,1,1,0]: \n",
    "        return [header_group[0], header_group[1], header_group[3], header_group[4]]\n",
    "    else:\n",
    "        return header_group\n",
    "\n",
    "headers = []\n",
    "chunk_forewards = []\n",
    "chunk_contents = []\n",
    "marker_string_indices = marker_string_indices\n",
    "for key, index in enumerate(marker_string_indices):\n",
    "    #print(lines[index-5:index+5])\n",
    "    header_group = lines[index-5:index+1]\n",
    "    header_group = [i if not(\" + \" in i) else \"\\n\" for i in header_group]\n",
    "    \n",
    "    header_group.reverse()\n",
    "\n",
    "    temp = [header_group[0]]\n",
    "    slashn_reached = 0\n",
    "    items_added = 1\n",
    "\n",
    "    ## Addressing each format individually,\n",
    "    ## this is messy but works most of the time\n",
    "    header_group = header_parse(header_group)\n",
    "    header_group.reverse()\n",
    "    headers.append(header_group)\n",
    "\n",
    "    if key == len(marker_string_indices) - 1:\n",
    "        chunk = lines[index+1:]\n",
    "    else:\n",
    "        chunk = lines[index+1:marker_string_indices[key+1]+1]\n",
    "\n",
    "    try:\n",
    "        chunk_foreward, chunk_content = ut.chunk_parse_namebased(chunk)\n",
    "        chunk_forewards.append(chunk_foreward)\n",
    "        chunk_contents.append(chunk_content)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(key)\n",
    "        print(header_group)\n",
    "        2+\"s\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a94132495860d142598aaea285fc060a9e1832a45151c3a0d158e377765afe23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
